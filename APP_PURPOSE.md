This application provides an agentic AI platform that runs LLM-driven workflows, records detailed provenance for each model call and property-scoped action, and orchestrates background processing via RQ workers. It exposes web APIs and background workers, persists structured provenance and application data in a relational database (with Alembic-managed migrations), and uses Redis for job queuing. The system is containerized with Docker Compose for easy local development and deployment, and is designed to make LLM usage auditable, reproducible, and integrated into automated task pipelines.
